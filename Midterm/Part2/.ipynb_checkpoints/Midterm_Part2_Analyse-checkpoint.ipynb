{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 Analyse 1\n",
    "\n",
    "* Archive API;\n",
    "* Article Search API;\n",
    "* Books API;\n",
    "* Community API;\n",
    "\n",
    "\n",
    "1. Using the Archive API data, read all json files in year 2016 \n",
    "3. Each json file contain a key called 'keywords' contains the details of each document. \n",
    "4. Extract each of the 'keywords' data and write it in an excel sheet. also add the related '_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json,csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path_to_json='Data/Archive/2016'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#write the title to the csv file\n",
    "csv_out = open ('Archive.csv','w')\n",
    "mywriter = csv.writer(csv_out)\n",
    "mywriter.writerow(['ID','Name','Rank','is_Major'])\n",
    "csv_out.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for js in json_files:\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        alldata= json.load(json_file)\n",
    "        keywords = alldata['response']['docs']      \n",
    "        for one in keywords:\n",
    "            rankList=[]\n",
    "            is_majorList=[]\n",
    "            nameList =[]\n",
    "            idList=[]  \n",
    "            # get the data from each key\n",
    "            try:\n",
    "                for each in one['keywords']:   \n",
    "                    rankList.append(each['rank'])\n",
    "                    is_majorList.append(each['is_major'])\n",
    "                    nameList.append(each['name'])\n",
    "                idList = [one['_id']+'',] * (len(is_majorList))\n",
    "                \n",
    "                #write all the rows to the csv file using 'a' to append each restaurant\n",
    "                csv_out_res = open ('Archive.csv','a')\n",
    "                mywriter_res = csv.writer(csv_out_res)\n",
    "                for row in zip(idList,nameList,rankList,is_majorList):\n",
    "                    mywriter_res.writerow(row)              \n",
    "                csv_out_res.close()\n",
    "\n",
    "            except (KeyError):\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 Analyse 2\n",
    "\n",
    "* Archive API;\n",
    "* Article Search API;\n",
    "* Books API;\n",
    "* Community API;\n",
    "\n",
    "1. Using the Article Search API and Books API data, read all json files in 'ArticleSearch' and 'Books'\n",
    "\n",
    "2. In the Article Search API, each json file contain keys called 'main', for each president, find out the number of the 'main' that contains  president's name\n",
    "\n",
    "3. In the Books API, each json file contain a key 'title', for each president, find out the number of titles that contains his name. and compare with 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_Article='Data/ArticleSearch/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_Article) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the function to contain the presidents and their counts\n",
    "def preMain():\n",
    "    preMainlist=[]\n",
    "    for js in json_files:\n",
    "        newjs= js[:-5]\n",
    "        with open(os.path.join(path_to_Article, js)) as json_file:\n",
    "            alldata= json.load(json_file)\n",
    "            keywords = alldata['response']['docs']\n",
    "            \n",
    "            mainList = []\n",
    "            for one in keywords:          \n",
    "                main = one['headline']['main'].lower()       \n",
    "                if main.find(newjs.lower()) != -1:\n",
    "                    mainList.append(main)\n",
    "            preMain ={}\n",
    "            preMain[newjs] = len(mainList)\n",
    "            preMainlist.append(preMain)\n",
    "    return(preMainlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Adams': 7}, {'Buchanan': 0}, {'Bush': 1}, {'Carter': 5}, {'Cleveland': 4}, {'Clinton': 2}, {'Coolidge': 6}, {'Eisenhower': 3}, {'Garfield': 2}, {'Grant': 4}, {'Harding': 8}, {'Harrison': 3}, {'Hayes': 5}, {'Hoover': 7}, {'Jackson': 4}, {'Jefferson': 3}, {'Johnson': 3}, {'Kennedy': 1}, {'Lincoln': 7}, {'Madison': 1}, {'McKinley': 2}, {'Monroe': 1}, {'Nixon': 0}, {'Obama': 1}, {'Pierce': 5}, {'Polk': 3}, {'Reagan': 0}, {'Roosevelt': 5}, {'Taft': 2}, {'Taylor': 0}, {'Truman': 5}, {'VanBuren': 5}, {'Washington': 2}, {'Wilson': 4}]\n"
     ]
    }
   ],
   "source": [
    "#get the freq of each preseidents in articles\n",
    "print(preMain())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_books='Data/Books/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_books) if pos_json.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# because when get the data ,it pass the value of the name of each presidents,\n",
    "# every title contains the name of the presidents.\n",
    "def preTitle():\n",
    "    preTitleList=[]\n",
    "    for js in json_files:\n",
    "        newjs= js[:-5]\n",
    "        with open(os.path.join(path_to_books, js)) as jf:\n",
    "            alldata= json.load(jf)\n",
    "\n",
    "            titleList=[]\n",
    "            results = alldata['results']    \n",
    "            for one in results: \n",
    "                title = (one['title'])\n",
    "                titleList.append(title)\n",
    "                \n",
    "            preBooks ={}\n",
    "            preBooks[newjs] = len(titleList)\n",
    "            preTitleList.append(preBooks)\n",
    "            \n",
    "    return(preTitleList) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Adams': 10}, {'Buchanan': 1}, {'Bush': 20}, {'Carter': 8}, {'Cleveland': 3}, {'Clinton': 20}, {'Coolidge': 2}, {'Eisenhower': 9}, {'Garfield': 1}, {'Grant': 20}, {'Harding': 0}, {'Harrison': 3}, {'Hayes': 2}, {'Hoover': 2}, {'Jackson': 20}, {'Jefferson': 20}, {'Johnson': 15}, {'Kennedy': 20}, {'Lincoln': 20}, {'Madison': 8}, {'McKinley': 1}, {'Monroe': 7}, {'Nixon': 19}, {'Obama': 20}, {'Pierce': 2}, {'Polk': 2}, {'Reagan': 20}, {'Roosevelt': 20}, {'Taft': 2}, {'Taylor': 5}, {'Truman': 6}, {'VanBuren': 0}, {'Washington': 20}, {'Wilson': 14}]\n"
     ]
    }
   ],
   "source": [
    "#get the freq of each preseidents in books\n",
    "print(preTitle())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "* the number of preseidents' name at book API 'title' key are more than the Article Search API 'main' key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part2 Analyse 3\n",
    "\n",
    "* Archive API;\n",
    "* Article Search API;\n",
    "* Books API;\n",
    "* Community API;\n",
    "\n",
    "\n",
    "1. Using the Community API read all json files in 'Community/'\n",
    "\n",
    "2. In the Community API, each json file contain a key called 'userID', find out the number of unique Users in year 2014, 2015 and 2016; and compare the change trend.\n",
    "\n",
    "3. In the Community API, each json file contain a key called 'commentType', find out the number of each commentType. compare the three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the function to get the user pass different year\n",
    "def getCommuUers(year):\n",
    "    path_to_commu='Data/Community/'+str(year)+'_user/'\n",
    "    json_files_commu = [pos_json for pos_json in os.listdir(path_to_commu) if pos_json.endswith('.json')]\n",
    "    userIdList=[]\n",
    "    for js in json_files_commu:\n",
    "        with open(os.path.join(path_to_commu, js)) as json_file:\n",
    "            alldata= json.load(json_file)               \n",
    "            try:\n",
    "                comments = alldata['results']['comments']\n",
    "                for one in comments:\n",
    "                    userIdList.append(one['userID'])\n",
    "            except (KeyError):\n",
    "                pass \n",
    "    print(len(set(userIdList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5163\n",
      "5220\n",
      "4628\n"
     ]
    }
   ],
   "source": [
    "getCommuUers(2016)\n",
    "getCommuUers(2015)\n",
    "getCommuUers(2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "* compared with 2014, the user increased by around 600, however in 2016 it is slightly cutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getComent(year):\n",
    "    path_to_commu='Data/Community/'+ str(year) +'_user/'\n",
    "    json_files_commu = [pos_json for pos_json in os.listdir(path_to_commu) if pos_json.endswith('.json')]\n",
    "    #get the List of \n",
    "    commentType =[]\n",
    "    for js in json_files_commu:\n",
    "        with open(os.path.join(path_to_commu, js)) as json_file:\n",
    "            alldata= json.load(json_file)  \n",
    "            try:\n",
    "                comments = alldata['results']['comments']\n",
    "                for one in comments:\n",
    "                    commentType.append(one['commentType'])\n",
    "            except (KeyError):\n",
    "                pass\n",
    "            \n",
    "    return(commentType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "List2014=getComent(2014)\n",
    "List2015=getComent(2015)\n",
    "List2016=getComent(2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFreq(myList):\n",
    "    typeFreq ={}\n",
    "    for one in myList:\n",
    "        if one not in typeFreq:\n",
    "            typeFreq[one] =1  #give the value 1 if it is the new key\n",
    "        else:\n",
    "            typeFreq[one] +=1 #if it is not the new key then accumulate the value\n",
    "    #sort the word-freq using lambda.\n",
    "    typeFreq = sorted(typeFreq.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return typeFreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment', 4936), ('userReply', 3913), ('reporterReply', 1)]"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFreq(List2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment', 4792), ('userReply', 4254), ('reporterReply', 4)]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFreq(List2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('comment', 4810), ('userReply', 4265)]"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFreq(List2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8850\n",
      "9050\n",
      "9075\n"
     ]
    }
   ],
   "source": [
    "# get the total comments amount for each user.\n",
    "print(len(List2014))\n",
    "print(len(List2015))\n",
    "print(len(List2016))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "* from the 2014 to 2016 the amount of comments is also increased, and the user usually used to comment and replay the comment."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
